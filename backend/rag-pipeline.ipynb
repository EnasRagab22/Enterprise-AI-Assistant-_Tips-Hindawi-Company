{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14663321,"sourceType":"datasetVersion","datasetId":9367557}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:46:56.871420Z","iopub.execute_input":"2026-01-29T16:46:56.871750Z","iopub.status.idle":"2026-01-29T16:46:57.172697Z","shell.execute_reply.started":"2026-01-29T16:46:56.871720Z","shell.execute_reply":"2026-01-29T16:46:57.172029Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/company-data/data/hr_policy_ar.txt\n/kaggle/input/company-data/data/tips_hindawi_data_ar.txt\n/kaggle/input/company-data/data/support_tickets_en.csv\n/kaggle/input/company-data/data/leave_policy_ar.txt\n/kaggle/input/company-data/data/leave_policy_en.pdf\n/kaggle/input/company-data/data/internal_faqs_ar.txt\n/kaggle/input/company-data/data/tips_hindawi_data_en.txt\n/kaggle/input/company-data/data/it_guidelines_en.txt\n/kaggle/input/company-data/data/internal_faqs_en.txt\n/kaggle/input/company-data/data/support_tickets_ar.csv\n/kaggle/input/company-data/data/hr_policy.pdf\n/kaggle/input/company-data/data/it_guidelines_ar.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q fastapi uvicorn pyngrok transformers==4.52.4 accelerate faiss-cpu langchain langchain-community pypdf sentence-transformers langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:46:57.173935Z","iopub.execute_input":"2026-01-29T16:46:57.174298Z","iopub.status.idle":"2026-01-29T16:47:19.518514Z","shell.execute_reply.started":"2026-01-29T16:46:57.174246Z","shell.execute_reply":"2026-01-29T16:47:19.517559Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:47:19.519730Z","iopub.execute_input":"2026-01-29T16:47:19.519974Z","iopub.status.idle":"2026-01-29T16:47:19.921923Z","shell.execute_reply.started":"2026-01-29T16:47:19.519941Z","shell.execute_reply":"2026-01-29T16:47:19.921105Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da68e33a01d2458ba1fb789fea81baee"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nprint(\"ü¶ô Loading Meta Llama 3 8B Instruct...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True\n).eval()\n\nprint(\"Llama 3 loaded successfully!\")\n\n\ndef generate_text(prompt: str, max_length: int = None) -> str:\n    \"\"\"Generate text using Llama 3 with chat template\"\"\"\n    if max_length is None:\n        max_length = 1000\n    \n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    formatted_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_length,\n            pad_token_id=tokenizer.eos_token_id,\n            do_sample=True,\n            top_k=50,\n            top_p=0.9,\n            temperature=0.7\n        )\n    \n    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    if \"assistant\" in full_response.lower():\n        parts = full_response.split(\"assistant\")\n        answer = parts[-1].strip()\n    else:\n        answer = full_response.strip()\n    \n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:47:32.177999Z","iopub.execute_input":"2026-01-29T16:47:32.178284Z","iopub.status.idle":"2026-01-29T16:50:35.874207Z","shell.execute_reply.started":"2026-01-29T16:47:32.178238Z","shell.execute_reply":"2026-01-29T16:50:35.873341Z"}},"outputs":[{"name":"stdout","text":"ü¶ô Loading Meta Llama 3 8B Instruct...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"783541c9d0364707a90fc20a20f622cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a98e46058bd04587bcf257abfea4070f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f502a513de04b1aa4868cfc4e673eda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d84d608f5bff4c35858d562d61b6f377"}},"metadata":{}},{"name":"stderr","text":"2026-01-29 16:47:42.523825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769705262.762287      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769705262.831168      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769705263.399063      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769705263.399094      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769705263.399097      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769705263.399100      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34401ee31dec44f5848907852240bbf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f6b309740ae4859a26bb833d6fb7221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfe0a1d7960b4619a2ed090476f85e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76ff832c44fa446ebaa43a54e42e5d13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"823bde1b1b9449848e7cd7c352806a28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e02f8d168a4d1bb09f2fa6d09a6e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3faa0c6074964df0af6b65fffa0aaf4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"180fb0d7d102463ebec421dfc3b9db60"}},"metadata":{}},{"name":"stdout","text":"Llama 3 loaded successfully!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.document_loaders import PyPDFLoader, TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nDATA_DIR = \"/kaggle/input/company-data/data\" \ndocuments = []\nfile_count = 0\n\nfor filename in os.listdir(DATA_DIR):\n    filepath = os.path.join(DATA_DIR, filename)\n    \n    try:\n        if filename.endswith('.pdf'):\n            loader = PyPDFLoader(filepath)\n            file_count += 1\n        elif filename.endswith('.txt'):\n            loader = TextLoader(filepath, encoding='utf-8')\n            file_count += 1\n        else:\n            continue\n        \n        docs = loader.load()\n        documents.extend(docs)\n        print(f\"   ‚úì Loaded: {filename}\")\n    \n    except Exception as e:\n        print(f\"   ‚úó Error loading {filename}: {str(e)}\")\n\nprint(f\"\\nLoaded {len(documents)} document(s) from {file_count} file(s)\")\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=500,\n    chunk_overlap=100,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n)\nchunks = text_splitter.split_documents(documents)\nprint(f\"Created {len(chunks)} chunks\")\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n)\nvectordb = FAISS.from_documents(chunks, embeddings)\nprint(\"‚úÖ Vector store ready!\")\n\n\ndef search_documents(query: str, k: int = None):\n    \"\"\"Search for relevant documents\"\"\"\n    if k is None:\n        k = 3\n    return vectordb.similarity_search(query, k=k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:50:35.875985Z","iopub.execute_input":"2026-01-29T16:50:35.876624Z","iopub.status.idle":"2026-01-29T16:50:50.492439Z","shell.execute_reply.started":"2026-01-29T16:50:35.876593Z","shell.execute_reply":"2026-01-29T16:50:50.491492Z"}},"outputs":[{"name":"stdout","text":"   ‚úì Loaded: hr_policy_ar.txt\n   ‚úì Loaded: tips_hindawi_data_ar.txt\n   ‚úì Loaded: leave_policy_ar.txt\n   ‚úì Loaded: leave_policy_en.pdf\n   ‚úì Loaded: internal_faqs_ar.txt\n   ‚úì Loaded: tips_hindawi_data_en.txt\n   ‚úì Loaded: it_guidelines_en.txt\n   ‚úì Loaded: internal_faqs_en.txt\n   ‚úì Loaded: hr_policy.pdf\n   ‚úì Loaded: it_guidelines_ar.txt\n\nLoaded 10 document(s) from 10 file(s)\nCreated 28 chunks\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2207542100.py:40: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a23216b96614590a8b745376c4f3f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ca89bb08124f7ab6335f98816c4a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfd0a63496b94c139902b5a27bdb6e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30b230021427497184b90425d954e6b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd83120bc884e748f8d6513d68c6b18"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34bfc1f9d8fc413d875f89ec92471fe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/526 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e10be17c6241b8a26c6ebd31109f9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"965bee60e56048c4aed25d3a02f05ea8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c92520af0b4de68f51263ea90a9380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c92fba4c4349ab83297c8385d77540"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Vector store ready!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from langdetect import detect\n\ndef detect_language(text: str) -> str:\n    \"\"\"Detect if text is Arabic or English\"\"\"\n    try:\n        lang = detect(text)\n        return lang\n    except:\n        return \"en\"\n\n\ndef create_rag_prompt(query: str, context: str) -> str:\n    \"\"\"Create RAG prompt optimized for Llama 3 with multilingual support\"\"\"\n    \n    # Detect language\n    lang = detect_language(query)\n    \n    if lang == \"ar\":\n        # Arabic prompt\n        return f\"\"\"ÿ£ŸÜÿ™ ŸÖÿ≥ÿßÿπÿØ ÿ∞ŸÉÿßÿ° ÿßÿµÿ∑ŸÜÿßÿπŸä ŸÖŸÅŸäÿØ ŸÑŸÑÿ¥ÿ±ŸÉÿßÿ™. ŸÖŸáŸÖÿ™ŸÉ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑŸÖŸàÿ∏ŸÅŸäŸÜ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸÜ Ÿàÿ´ÿßÿ¶ŸÇ ÿßŸÑÿ¥ÿ±ŸÉÿ© ÿßŸÑŸÖŸÇÿØŸÖÿ© ÿ£ÿØŸÜÿßŸá ŸÅŸÇÿ∑.\n\nÿ™ÿπŸÑŸäŸÖÿßÿ™ ŸÖŸáŸÖÿ©:\n- ÿ£ÿ¨ÿ® ŸÅŸÇÿ∑ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÖŸÜ ÿßŸÑÿ≥ŸäÿßŸÇ ÿ£ÿØŸÜÿßŸá\n- ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™ŸÉŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ŸÅŸä ÿßŸÑÿ≥ŸäÿßŸÇÿå ŸÇŸÑ \"ŸÑŸäÿ≥ ŸÑÿØŸä ŸÖÿπŸÑŸàŸÖÿßÿ™ ŸÉÿßŸÅŸäÿ© ŸÑŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ§ÿßŸÑ ÿ®ŸÜÿßÿ°Ÿã ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑŸÖÿ™ÿßÿ≠ÿ©\"\n- ŸÉŸÜ ŸÖŸàÿ¨ÿ≤ÿßŸã ŸàŸàÿßÿ∂ÿ≠ÿßŸã ŸàŸÖŸáŸÜŸäÿßŸã\n- ŸÑÿß ÿ™ÿÆÿ™ŸÑŸÇ ÿ£Ÿà ÿ™ŸÅÿ™ÿ±ÿ∂ ŸÖÿπŸÑŸàŸÖÿßÿ™\n\nÿßŸÑÿ≥ŸäÿßŸÇ ŸÖŸÜ Ÿàÿ´ÿßÿ¶ŸÇ ÿßŸÑÿ¥ÿ±ŸÉÿ©:\n{context}\n\nÿ≥ÿ§ÿßŸÑ ÿßŸÑŸÖŸàÿ∏ŸÅ: {query}\n\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\"\"\"\n    else:\n        # English prompt\n        return f\"\"\"You are a helpful enterprise AI assistant. Your job is to answer employee questions using only the information from company documents provided below.\n\nIMPORTANT INSTRUCTIONS:\n- Answer ONLY using information from the context below\n- If the answer is not in the context, say \"I don't have enough information to answer this based on the available documents\"\n- Be concise, clear, and professional\n- Do not make up or assume information\n- CRITICAL: Respond in the SAME language as the question\n\nCONTEXT FROM COMPANY DOCUMENTS:\n{context}\n\nEMPLOYEE QUESTION: {query}\n\nANSWER:\"\"\"\n\n\ndef ask_question(question: str, max_length: int = None) -> dict:\n    \"\"\"Main RAG pipeline with Llama 3 - Now supports Arabic and English\"\"\"\n    \n    # Step 1: Retrieve relevant documents\n    docs = search_documents(question)\n    \n    # Step 2: Combine context\n    context = \"\\n\\n\".join([\n        f\"[Document {i+1}]\\n{doc.page_content}\" \n        for i, doc in enumerate(docs)\n    ])\n    \n    # Step 3: Create prompt (automatically detects language)\n    prompt = create_rag_prompt(question, context)\n    \n    # Step 4: Generate answer\n    answer = generate_text(prompt, max_length)\n    \n    # Step 5: Clean answer\n    if \"ANSWER:\" in answer or \"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\" in answer:\n        if \"ANSWER:\" in answer:\n            answer = answer.split(\"ANSWER:\")[-1].strip()\n        elif \"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\" in answer:\n            answer = answer.split(\"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©:\")[-1].strip()\n    \n    return {\n        \"question\": question,\n        \"answer\": answer,\n        \"language\": detect_language(question)\n    }\n\nprint(\"‚úÖ RAG Pipeline ready with Arabic and English support!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:50:50.493562Z","iopub.execute_input":"2026-01-29T16:50:50.493917Z","iopub.status.idle":"2026-01-29T16:50:50.521496Z","shell.execute_reply.started":"2026-01-29T16:50:50.493863Z","shell.execute_reply":"2026-01-29T16:50:50.520601Z"}},"outputs":[{"name":"stdout","text":"‚úÖ RAG Pipeline ready with Arabic and English support!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from fastapi import FastAPI, HTTPException, Header\nfrom pydantic import BaseModel\nfrom typing import Optional\n\nAPI_KEY = \"secret226\"\nNGROK_TOKEN = \"3825DpWFqNVWaCztnWjTVZAZDw1_3LoS6zTvipBMY6HYoCV8k\"\n\napp = FastAPI(\n    title=\"ü¶ô Enterprise AI Assistant - Llama 3\",\n    description=\"RAG-based Q&A system powered by Meta Llama 3 8B Instruct with Arabic & English support\",\n    version=\"2.0.0\"\n)\n\n\nclass QueryRequest(BaseModel):\n    question: str\n    max_length: Optional[int] = None\n\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"message\": \"ü¶ô Enterprise AI Knowledge Assistant API\",\n        \"model\": \"Meta-Llama-3-8B-Instruct\",\n        \"status\": \"running\",\n        \"version\": \"2.0.0\",\n        \"languages\": [\"English\", \"Arabic\"],\n        \"endpoints\": {\n            \"ask\": \"/ask - RAG-based Q&A\",\n            \"generate\": \"/generate - Direct generation\",\n            \"health\": \"/health - Health check\"\n        }\n    }\n\n\n@app.get(\"/health\")\nasync def health():\n    return {\n        \"status\": \"healthy\",\n        \"model\": \"Meta-Llama-3-8B-Instruct\",\n        \"languages_supported\": [\"en\", \"ar\"]\n    }\n\n\n@app.post(\"/ask\")\nasync def api_ask(request: QueryRequest, authorization: str = Header(...)):\n    \"\"\"Ask a question with RAG - Supports Arabic and English\"\"\"\n    # Verify API key\n    if not authorization.startswith(\"Bearer \"):\n        raise HTTPException(status_code=401, detail=\"Invalid authorization format\")\n    \n    token = authorization.replace(\"Bearer \", \"\")\n    if token != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    \n    # Process question\n    try:\n        result = ask_question(request.question, request.max_length)\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\n@app.post(\"/generate\")\nasync def api_generate(request: QueryRequest, authorization: str = Header(...)):\n    \"\"\"Direct text generation - Supports Arabic and English\"\"\"\n    # Verify API key\n    if not authorization.startswith(\"Bearer \"):\n        raise HTTPException(status_code=401, detail=\"Invalid authorization format\")\n    \n    token = authorization.replace(\"Bearer \", \"\")\n    if token != API_KEY:\n        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n    \n    try:\n        response = generate_text(request.question, request.max_length)\n        return {\"response\": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n\nprint(\"‚úÖ FastAPI app configured for Llama 3 with multilingual support\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:50:50.522622Z","iopub.execute_input":"2026-01-29T16:50:50.522847Z","iopub.status.idle":"2026-01-29T16:50:51.826410Z","shell.execute_reply.started":"2026-01-29T16:50:50.522825Z","shell.execute_reply":"2026-01-29T16:50:51.825781Z"}},"outputs":[{"name":"stdout","text":"‚úÖ FastAPI app configured for Llama 3 with multilingual support\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import uvicorn\nimport threading\nimport socket\nfrom pyngrok import ngrok, conf\nimport time\n\n\ndef get_free_port():\n    \"\"\"Find available port\"\"\"\n    s = socket.socket()\n    s.bind(('', 0))\n    port = s.getsockname()[1]\n    s.close()\n    return port\n\n\nport = get_free_port()\n\nconf.get_default().auth_token = NGROK_TOKEN\n\npublic_url = ngrok.connect(port).public_url\nprint(f\"\\nüåê Public URL: {public_url}\")\n\n\ndef run_server():\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=port,\n        log_level=\"info\",\n        access_log=False\n    )\n\n\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()\n\nprint(\"\\n‚úÖ Server is running with Arabic support!\")\n\n# Keep the notebook alive\ntry:\n    while True:\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    print(\"\\n‚èπÔ∏è Server stopped\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T16:50:51.827270Z","iopub.execute_input":"2026-01-29T16:50:51.827564Z","execution_failed":"2026-01-29T17:49:05.789Z"}},"outputs":[{"name":"stdout","text":"                                                                                                    \nüåê Public URL: https://thigmotropic-geraldine-nonelliptically.ngrok-free.dev\n\n‚úÖ Server is running with Arabic support!\n","output_type":"stream"},{"name":"stderr","text":"INFO:     Started server process [55]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:49077 (Press CTRL+C to quit)\n","output_type":"stream"}],"execution_count":null}]}